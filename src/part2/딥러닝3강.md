## [오전강의]

### 🎯 주제

- 텐서 연산의 기본 (Indexing, Slicing, 결합, 변형)과 신경망 내에서의 연산 원리

### 📝 핵심개념 정리

1. **텐서 연산의 중요성**

   - 텐서 연산은 파이토치의 핵심으로, 신경망 학습의 기본 단위가 됨
   - ⭐ 신경망은 텐서와 연산의 조합으로 구성됨
   - 순전파(forward propagation)와 역전파(back propagation)는 모두 연산으로 이루어짐
2. **신경망의 기본 원리**

   - 신경망은 층(layer)마다 내적 연산이 이루어지는 구조
   - 내적은 상관관계를 구하는 연산으로, 신경망의 기본 작동 방식
   - ⭐ 역전파는 학습의 핵심으로, 오차를 최소화하여 예측 정확도를 높임
3. **Indexing과 Slicing**

   - Indexing: 단일 값 추출 방식
   - Slicing: 여러 개 값 추출 방식
   - numpy 스타일과 유사하지만 판다스와는 다름
   - ⭐ 주의: 슬라이싱 결과는 단일값이 나와도 2차원 텐서로 유지됨
4. **불린 인덱싱(Boolean Indexing)**

   - 조건에 맞는 요소만 추출하는 방식
   - 마스크(조건식)를 만들어 적용
   - 결과는 1차원으로 반환되므로 필요시 reshape 필요
   - 이미지나 음성 데이터에서 특정 조건의 데이터만 추출할 때 유용
5. **텐서 결합 연산: cat과 stack**

   - torch.cat: 기존 차원을 확장하여 텐서를 이어붙임
   - 붙이려는 차원을 제외한 다른 차원의 크기는 동일해야 함
   - torch.stack: 새로운 차원을 추가하여 텐서를 쌓음
   - ⭐ stack은 모든 텐서의 모양이 완전히 동일해야 함
   - stack은 차원 수가 1 증가하며, 배치 텐서를 만들 때 핵심적으로 사용됨
6. **차원 조정: squeeze와 unsqueeze**

   - squeeze: 크기가 1인 불필요한 차원을 제거
   - unsqueeze: 특정 위치에 크기가 1인 차원을 추가
   - ⭐ 신경망에서 레이어 간 차원을 맞추는 데 필수적으로 사용
   - 차원 불일치는 신경망에서 가장 많이 발생하는 오류 중 하나
7. **합성곱(Convolution) 개념**

   - 데이터를 겹치게(overlap) 처리하는 방식
   - CNN(Convolutional Neural Network)에서 핵심적으로 사용
   - 자연어 처리에서는 문맥 정보를 유지하기 위해 토큰 겹침 활용
   - 이미지 처리에서는 전체적인 특성을 파악하기 위해 영역 겹침 활용

## [오후강의]

### 🎯 주제

* PyTorch 텐서 조작 및 연산 방법(squeeze/unsqueeze, view/reshape, 요소별 연산, 행렬 연산, 브로드캐스팅, numpy와의 호환성)

### 📝 핵심개념 정리

1. Squeeze와 Unsqueeze
   * Squeeze: 크기가 1인 차원을 삭제하는 연산
     * 일반적으로 `squeeze()` 함수 사용 시 모든 크기가 1인 차원을 제거
     * `dim` 파라미터로 특정 위치의 차원만 제거 가능 (예: `squeeze(dim=0)`)
     * ⭐ 크기가 1이 아닌 차원은 삭제되지 않음 [03:29]
   * Unsqueeze: 특정 위치에 크기가 1인 차원을 추가하는 연산
     * `dim` 파라미터로 추가할 위치 지정 (예: `unsqueeze(dim=0)`)
     * 주로 행렬 연산 시 차원을 맞추기 위해 사용 [16:18]
   * 신경망에서는 텐서의 차원이 맞지 않을 때 내적 연산을 위해 자주 사용됨 [17:18]
2. View와 Reshape
   * 두 함수 모두 텐서의 모양(shape)을 변경하지만 차이점이 있음
   * View
     * 텐서의 모양을 바꾸지만 메모리에 저장된 데이터 순서는 변경하지 않음 [33:53]
     * 원본 텐서와 메모리를 공유하므로 데이터 복사가 발생하지 않아 빠름
     * ⭐ 텐서가 메모리상에서 연속적일 때만 작동함 [33:53]
     * 원본 데이터를 바라보는 창(window)의 역할 [34:52]
   * Reshape
     * 텐서가 비연속적인 경우에도 사용 가능 [37:49]
     * 자동으로 데이터를 복사해서 연속적으로 만든 후 모양을 변경 [37:49]
     * 안전하고 편리하지만 의도치 않은 데이터 복사가 발생할 수 있음
   * ⭐ 연속적인 텐서는 view를 사용하면 메모리와 속도 측면에서 유리함 [66:07]
   * 비연속적인 텐서(transpose나 permute 후)는 reshape을 사용해야 함 [65:10]
3. 텐서 연산 - 요소별(Element-wise) 연산
   * 텐서의 같은 위치에 있는 요소끼리 연산 수행
   * 덧셈(`+`), 뺄셈(`-`), 곱셈(`*`), 나눗셈(`/`), 거듭제곱(`**`) 등 산술 연산
   * 연산에 참여하는 텐서들의 모양이 같아야 함 (브로드캐스팅 가능 시 예외)
   * PyTorch 함수로도 사용 가능: `torch.add(a, b)` = `a + b`
4. Reduction 연산
   * 텐서의 여러 개 값을 하나의 스칼라 값으로 줄이는 연산 [94:58]
   * 주요 함수: `sum()`, `mean()`, `max()`, `min()`
   * 텐서 전체에 대해 수행하거나 특정 차원(dim)에 따라 수행 가능
   * 행렬에서 dim=0은 열 방향으로, dim=1은 행 방향으로 연산 [99:53]
   * ⭐ 신경망에서 자주 사용되는 연산 [98:56]
5. 행렬 연산
   * 행렬 곱셈(내적): 신경망의 기본 연산 [103:47]
   * 두 가지 방법으로 수행:
     * `torch.matmul(mat1, mat2)` 함수 사용
     * `@` 연산자 사용 (`mat1 @ mat2`)
   * ⭐ 행렬 곱셈은 첫 번째 텐서의 열 개수와 두 번째 텐서의 행 개수가 같아야 함 [104:46]
   * 결과 텐서의 크기는 첫 번째 텐서의 행 개수 × 두 번째 텐서의 열 개수
6. In-place 연산
   * 함수나 메서드 이름 끝에 언더스코어(`_`)가 붙은 버전 (예: `add_`)
   * 새로운 텐서를 반환하지 않고 텐서 자체를 직접 수정함 [110:47]
   * 장점: 메모리를 새로 할당하지 않아 메모리 절약과 속도 향상 [111:41]
   * ⭐ 단점: 자동미분(autograd)에 문제를 일으킬 수 있음 [111:41]
   * ⭐ 경사 계산에 필요한 텐서(requires_grad=True)에는 사용하지 않는 것이 안전 [111:41]
7. 브로드캐스팅(Broadcasting)
   * 모양이 다른 텐서 간의 산술 연산을 가능하게 하는 메커니즘 [139:40]
   * 규칙:
     * 두 텐서의 차원을 뒤에서부터 앞으로 비교
     * 각 차원의 크기가 동일하거나, 한쪽 텐서의 해당 차원 크기가 1이거나, 해당 차원이 없어야 함 [140:39]
   * ⭐ 브로드캐스팅은 텐서를 실제로 복사하지 않고 가상으로 확장하여 연산 [140:39]
   * 코드를 간결하게 만들고 불필요한 메모리 사용을 줄임
   * 스칼라 값과 텐서 연산 시 자주 사용됨 [149:29]
8. NumPy와의 호환성
   * PyTorch는 NumPy와 긴밀하게 통합되어 있음
   * NumPy 배열과 PyTorch 텐서 간 변환 방법:
     * `torch.from_numpy(np_array)`: NumPy 배열을 텐서로 변환 (메모리 공유) [173:23]
     * `tensor.numpy()`: 텐서를 NumPy 배열로 변환 (메모리 공유) [180:06]
   * ⭐ 메모리 공유의 제약사항:
     * CPU 텐서에서만 가능 (GPU 텐서는 먼저 `.cpu()` 호출 필요) [182:04]
     * 데이터 타입 주의: NumPy는 기본적으로 float64, PyTorch는 float32 [183:04]
   * 장점: 메모리 효율성과 속도 [186:57]
   * 주의점: 한쪽에서의 변경이 다른 쪽에도 반영됨 [179:09]
