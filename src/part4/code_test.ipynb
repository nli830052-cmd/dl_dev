{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8cf3c129",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b262f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLinearModel(nn.Module):\n",
    "    def __init__(self,input_size,output_size):\n",
    "        super(SimpleLinearModel,self).__init__()\n",
    "        self.linear = nn.Linear(in_features=input_size,out_features=output_size)\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        output=self.linear(x)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e68e178",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 신경망 구성 : class\n",
    "class SimpleLinearModel(nn.Module):\n",
    "    # __init__ : 부품(레이어) 생성,초기화\n",
    "    def __init__(self,input_size,output_size):\n",
    "        # 1. 부모 클래스인 Module의 생성자를 반드시 먼저 호출해야함, 안하면 에러 발생\n",
    "        #()의미:생성하고 호출해라(()없으면 super 가 생성이 안됨)\n",
    "        #SimpleLinearModel위에서 선언한 class 명 적어야해\n",
    "        super(SimpleLinearModel,self).__init__()\n",
    "\n",
    "\n",
    "        # 레이어 생성: self. --> 오브젝트 안에다 만들겠다.\n",
    "        # 레이어(부품) 생성\n",
    "        self.linear = nn.Linear(in_features=input_size,out_features=output_size) # y = w*X + b\n",
    "        # w가 나올려면 input_size 가 필요하고, y가 나올려면 output_size 가 필요함\n",
    "\n",
    "\n",
    "    # forward override : 부품 조립(=신경망 만든다)\n",
    "    def forward(self,x): # x 입력데이터 받게 해야함\n",
    "        output = self.linear(x)\n",
    "        return output # output 결과(예측값)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f9072c",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 10\n",
    "output_dim = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d96cb0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 오브젝트 생성\n",
    "input_dim = 10\n",
    "output_dim = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a781f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleLinearModel(input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82659063",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleLinearModel(input_size=input_dim, output_size=output_dim) # 오브젝트 생성(신경망 생성)\n",
    "                            # __init__도 실행됨 따라서 __init__(self,input_size,output_size) 에서 input_size,output_size도 넣어줘야함\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7d840b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleLinearModel(\n",
      "  (linear): Linear(in_features=10, out_features=5, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 3.모델 구조 출력\n",
    "print(model)\n",
    "\n",
    "# 딕셔너리 형태로 출력\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d49da7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name:linear.weight\n",
      "Shape:torch.Size([5, 10])\n",
      "Requires Grad :True\n",
      "Name:linear.bias\n",
      "Shape:torch.Size([5])\n",
      "Requires Grad :True\n"
     ]
    }
   ],
   "source": [
    "# 4.모델 파라미터 확인\n",
    "for name, param in model.named_parameters(): # param:w(가중치)\n",
    "    print(f'Name:{name}') \n",
    "    print(f'Shape:{param.shape}')\n",
    "    print(f'Requires Grad :{param.requires_grad}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b67d845",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([4, 10])\n",
      "Output shape torch.Size([4, 5])\n"
     ]
    }
   ],
   "source": [
    "# 5. 가상 입력 데이터로 순전파 테스트\n",
    "batch_size = 4\n",
    "dummy_input = torch.randn(batch_size,input_dim) # 핵심: tensor 들어가야함\n",
    "                                                # 4x10으로 만들겠다.\n",
    "output = model(dummy_input) # model.forwad(self,x)\n",
    "                            # model을 호출하면 바로 forward 호출하게됨(forward단어 없어도 호출된거임!!<쭈의!!!!>)\n",
    "                        \n",
    "\n",
    "\n",
    "print(f'Input shape: {dummy_input.shape}')\n",
    "print(f'Output shape {output.shape}')\n",
    "\n",
    "# batch 사이즈만큼 아웃풋이 나옴, 결과가 행렬이 나옴\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e31920a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20251223\n",
    "# nn.Sequential: 신경망 모델 구축 , nn.Linear만 사용(1차원 데이터)\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915e8018",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력:28 X 28 이미지 입력 -> 784 x 1 차원으로 펼친 이미지로 만들어아야함\n",
    "# 왜?nn.Linear만 사용(1차원 데이터)하니까 1차원으로 만들어야함\n",
    "# 출력:10개로 분류\n",
    "\n",
    "input_size = 28*28\n",
    "output_size = 10\n",
    "hidden_size = 512\n",
    "\n",
    "# model_seq = nn.Sequential(                                  # 오브젝트 생성하고, () 사이에 층을 넣어야함\n",
    "#             # layers\n",
    "# )\n",
    "model_seq = nn.Sequential(\n",
    "    nn.Linear(input_size, hidden_size), #(748,512) : 모델 전체의 입력\n",
    "    nn.ReLU(),                         #활성화 함수: 신경망의 학습을 활성화함\n",
    "    nn.Linear(hidden_size,hidden_size), #(512,512) # 층마다 input,output 있다.\n",
    "    nn.ReLU(),                                       \n",
    "    nn.Linear(hidden_size,output_size)                         # 모델 전체의 출력 ,<주의>출력층에서 Relu 하면 안됨\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da488b8",
   "metadata": {},
   "source": [
    "<img src=\"./PyTorch nn.Sequentia.png\" alt=\"설명\" width=\"300\"/>\n",
    "<img src=\"./ReLU 함수의 수학적 그래프_ x축.png\" alt=\"설명\" width=\"500\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52995742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=512, out_features=10, bias=True)\n",
      ")\n",
      "---------------인덱스로 접근---------------\n",
      "Linear(in_features=784, out_features=512, bias=True)\n",
      "torch.Size([512, 784])\n"
     ]
    }
   ],
   "source": [
    "# bias는 디폴트가 True\n",
    "print(model_seq)\n",
    "print('---------------인덱스로 접근---------------')\n",
    "print(model_seq[0])\n",
    "print(model_seq[0].weight.shape) # 한층에만 파라미터가 40만개 나옴....., 신경망은 다 행렬 연산이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8126fa2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([64, 784])\n",
      "Output shape : torch.Size([64, 10])\n"
     ]
    }
   ],
   "source": [
    "# 가상 입력으로 테스트 , 배치 사이즈: 64,64장 이미지\n",
    "dummy_input = torch.randn(64,input_size)  # 64 X 784로 생성해라\n",
    "                                          # 64 : 배치사이즈 = 한번에 신경망에 들어가는 data즉, 한번에 64장을 신경망에 넣겠다.\n",
    "                                          # 여기서는 순전파만 하겠다.\n",
    "\n",
    "# 함수호출하는 연산자 , 여기서 ():특정함수 호출하는 행성자다\n",
    "# 포워드를 가져오는데 forward는 (self,x)를 받음 x =입력데이터\n",
    "# 따라서 dummpy_input = 입력데이터 가져오기\n",
    "\n",
    "output = model_seq(dummy_input)   \n",
    "\n",
    "\n",
    "print(f'Input shape: {dummy_input.shape}')\n",
    "print(f'Output shape : {output.shape}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15ab612",
   "metadata": {},
   "source": [
    "<img src=\"./PyTorch 모델에 가상 입력 데이.png\" alt=\"설명\" width=\"300\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4d7bdd0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=512, out_features=10, bias=True)\n",
      ")\n",
      "Linear(in_features=512, out_features=512, bias=True)\n",
      "torch.Size([512, 512])\n"
     ]
    }
   ],
   "source": [
    "# 특정 레이어에 접근하기\n",
    "print(model_seq)\n",
    "print(model_seq[2])\n",
    "print(model_seq[2].weight.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1b884a",
   "metadata": {},
   "source": [
    "<활성화함수>\n",
    "<img src=\"./ChatGPT Image 2025년 12월 23일 오전 11_58_51.png\" alt=\"설명\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "daa99e09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (input_layer): Linear(in_features=784, out_features=512, bias=True)\n",
      "  (relu1): ReLU()\n",
      "  (hidden_layer): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (relu2): ReLU()\n",
      "  (output_layer): Linear(in_features=512, out_features=10, bias=True)\n",
      ")\n",
      "Linear(in_features=512, out_features=512, bias=True)\n"
     ]
    }
   ],
   "source": [
    "# 층에 이름 부여\n",
    "from collections import OrderedDict\n",
    "model_ordered = nn.Sequential(\n",
    "    OrderedDict([ # 층들이 리스트 안에 튜플 형태로 들어감\n",
    "        ('input_layer',nn.Linear(input_size,hidden_size)), # Linear\n",
    "        ('relu1',nn.ReLU()), # ReLU\n",
    "        ('hidden_layer',nn.Linear(hidden_size,hidden_size)), # Linear\n",
    "        ('relu2',nn.ReLU()), # ReLU\n",
    "        ('output_layer',nn.Linear(hidden_size,output_size)), # Linear\n",
    "    ])\n",
    "\n",
    "\n",
    "\n",
    ")\n",
    "print(model_ordered)\n",
    "print(model_ordered.hidden_layer) # 이름으로 접근도 가능 \n",
    "                                  # 같은 의미 Model_seq[2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a8ef7054",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn.Module 상속해서 신경망 모델 구축\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F # 관례적으로 대문자F사용함\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "da84f758",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self,input_size,hidden_size,output_size): # 3개를 받아서 오브젝트 생성\n",
    "        '''\n",
    "        설명\n",
    "        '''\n",
    "        super(MLP,self).__init__() #super 가 nn.Module\n",
    "        # 사용될 레이어(부품)들을 정의\n",
    "        self.fc1 = nn.Linear(input_size,hidden_size) # 입력층\n",
    "        # self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size,hidden_size) # 중간층\n",
    "        # self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(hidden_size,output_size) # 출력층\n",
    "\n",
    "    def forward(self,x): # x :입력데이터 , forward안에서는 내 맘대로 할 수 있다.\n",
    "        # x = self.fc2(x) # 부품 만들었으니 내 마음대로 조정 가능함\n",
    "        # x = self.relu1(x)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)  # self.relu1 = nn.ReLU(), x = self.relu1(x) 안써도 됨\n",
    "        # x = self.relu1(x)\n",
    "        # x = self.fc2(x)\n",
    "        # x = self.relu2(x)\n",
    "        # x = self.fc3(x)\n",
    "        # x = self.relu3(x)\n",
    "        return x \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce708c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (fc1): Linear(in_features=784, out_features=512, bias=True)\n",
      "  (fc2): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (fc3): Linear(in_features=512, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 모델 오브젝트 생성 및 구조 출력\n",
    "# def __init__(self,input_size,hidden_size,output_size): # 3개를 받아서 오브젝트 생성\n",
    "# model_class = MLP(input_size=,output_size=) #포지셔널 아규먼트 방식으로 생성\n",
    "\n",
    "model_class = MLP(input_size=784,hidden_size=512,output_size=10) # 아규먼트만 생성하고 forward 는 생성 X\n",
    "# fcl : 키 , (fc1): Linear(in_features=784, out_features=512, bias=True) :value\n",
    "print(model_class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ca5c37ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가상 입력 데이터 생성, 순전파(forward)\n",
    "dummy_input = torch.randn(64,784) # 입력 데이터 생성\n",
    "\n",
    "# 순전파(forward): x가 output\n",
    "output = model_class(dummy_input)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2653f279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([64, 784])\n",
      "Output shape: torch.Size([64, 512])\n"
     ]
    }
   ],
   "source": [
    "print(f'Input shape: {dummy_input.shape}')\n",
    "print(f'Output shape: {output.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d795ea95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  nn.ModuleList\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da03389c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DynamicMLP NN 선언\n",
    "class DynamicMLP(nn.Module):\n",
    "    def __init__(self,input_size,hidden_size,output_size,num_hidden_layers):\n",
    "        super(DynamicMLP,self).__init__() # 얘가 빠지면 추적이 안된다.\n",
    "\n",
    "        self.input_layer = nn.Linear(input_size,hidden_size)#  입력층\n",
    "\n",
    "        # ModuleList\n",
    "        self.hidden_layers = nn.ModuleList(\n",
    "            [nn.Linear(hidden_size,hidden_size)for _ in range(num_hidden_layers)] \n",
    "            # _ : underscore: 받을 필요는 없지만 써야하는 것 , 많이 사용함\n",
    "            # num_hidden_layers = 3 # 생성될때 나옴\n",
    "\n",
    "         ) # 요놈이 생성한다\n",
    "\n",
    "        self.output_layer = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.input_layer(x)) # 2줄에 한줄에 끝남\n",
    "\n",
    "        # ModuleList : 간단하게 몇줄로 구성가능함 \n",
    "        # 파이썬 리스트는 gpu 접근이 어렵다.\n",
    "        for layer in self.hidden_layers:\n",
    "            x = F.relu(layer(x)) # 위에 \n",
    "\n",
    "\n",
    "        x = self.output_layer(x)\n",
    "        return x \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8254fe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DynamicMLP(\n",
      "  (input_layer): Linear(in_features=784, out_features=256, bias=True)\n",
      "  (hidden_layers): ModuleList(\n",
      "    (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)\n",
      "  )\n",
      "  (output_layer): Linear(in_features=256, out_features=10, bias=True)\n",
      ")\n",
      "ModuleList(\n",
      "  (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)\n",
      ")\n",
      "Linear(in_features=256, out_features=256, bias=True)\n"
     ]
    }
   ],
   "source": [
    "# 생성하고 구조학인(학습은 아직 X)\n",
    "model_dynamic = DynamicMLP(input_size=784,hidden_size=256,output_size=10, num_hidden_layers=3)\n",
    "print(model_dynamic)\n",
    "print(model_dynamic.hidden_layers)\n",
    "print(model_dynamic.hidden_layers[1]) # 인덱스 접근 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3d969cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conv2d\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d2a2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가상의 이미지 데이터 생성\n",
    "dummy_images = torch.randn(16,3,64,64) #(장수,칼라,높이,넓이)\n",
    "# 위에 가상이미지를 conv2에 집어 넣기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4fe5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력 채널 : 3\n",
    "# 출력 채널 : 32\n",
    "# 커널크기  : 3\n",
    "# 스트라이드: 1\n",
    "# 패딩:1\n",
    "conv_layer = nn.Conv2d(in_channels=3,out_channels=32,kernel_size=3,stride=1,padding=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9ba8ca35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([16, 3, 64, 64])\n",
      "Output shape:torch.Size([16, 32, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "conv_layer = nn.Conv2d(in_channels=3,out_channels=32,kernel_size=3,stride=1,padding=1)\n",
    "output_feature_map = conv_layer(dummy_images)\n",
    "print(f'Input shape: {dummy_images.shape}')\n",
    "print(f'Output shape:{output_feature_map.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e139f2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([16, 32, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 피처맵이 생성된 상태로 가정 : Conv2d의 결과\n",
    "feature_map = torch.randn(16,32,64,64)\n",
    "# 2 X 2 크기의 커널,스트라이드 2로 풀링\n",
    "# 영역안의 특징들만 추출해서 2x2로 만든다.\n",
    "pool_layer = nn.MaxPool2d(kernel_size=2,stride=2) # 생성만 한거이\n",
    "\n",
    "# 순전파 통과시켜야함\n",
    "output = pool_layer(feature_map)\n",
    "print(f'Output shape: {output.shape}')\n",
    "\n",
    "# feature_map = torch.randn(16,32,64,64) --> torch.Size([16, 32, 32, 32]) 로 반으로 줄음\n",
    "# 결론: pooling:반으로 줄음"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Docker Venv)",
   "language": "python",
   "name": "docker_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
