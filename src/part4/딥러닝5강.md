## [오전수업]

## 파이토치에서 신경망 구축 방법

## 핵심 개념

* 신경망 구축의 세 가지 방법: 기능과 복잡도에 따라 적절한 방법을 선택해야 함
  * 간단한 모델: NN.Sequential (가장 직관적)
  * 복잡한 모델: NN.Module 상속 (표준적이고 유연함)
  * 특수 경우: 함수형 접근 (파라미터가 없는 레이어)
* 신경망 구축의 핵심: 입력과 출력의 형태 결정이 가장 중요
* 신경망 내부는 모두 행렬 연산으로 처리됨

## 1. NN.Sequential 방식

* **개념** : 레이어를 순서대로 담는 컨테이너, 순차적으로 데이터가 통과함
* **특징** :
* 간결하고 직관적인 코드
* 분기나 건너뛰기 연결 불가능
* 프로토타이핑에 적합
* **사용 방법** :
  **python**copy.label

<pre><div class="text-03-R scrollbar"><span class="token plain">model_seq </span><span class="token operator">=</span><span class="token plain"> nn</span><span class="token punctuation">.</span><span class="token plain">Sequential</span><span class="token punctuation">(</span><span class="token plain"></span></div><div class="text-03-R scrollbar"><span class="token plain">    nn</span><span class="token punctuation">.</span><span class="token plain">Linear</span><span class="token punctuation">(</span><span class="token plain">input_size</span><span class="token punctuation">,</span><span class="token plain"> hidden_size</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token plain"></span></div><div class="text-03-R scrollbar"><span class="token plain">    nn</span><span class="token punctuation">.</span><span class="token plain">ReLU</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token plain"></span></div><div class="text-03-R scrollbar"><span class="token plain">    nn</span><span class="token punctuation">.</span><span class="token plain">Linear</span><span class="token punctuation">(</span><span class="token plain">hidden_size</span><span class="token punctuation">,</span><span class="token plain"> hidden_size</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token plain"></span></div><div class="text-03-R scrollbar"><span class="token plain">    nn</span><span class="token punctuation">.</span><span class="token plain">ReLU</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token plain"></span></div><div class="text-03-R scrollbar"><span class="token plain">    nn</span><span class="token punctuation">.</span><span class="token plain">Linear</span><span class="token punctuation">(</span><span class="token plain">hidden_size</span><span class="token punctuation">,</span><span class="token plain"> output_size</span><span class="token punctuation">)</span><span class="token plain"></span></div><div class="text-03-R scrollbar"><span class="token plain"></span><span class="token punctuation">)</span></div></pre>

* **레이어 접근** : 인덱스로 접근 가능 (model_seq[0])
* **이름 부여 방식** : OrderedDict 사용하여 의미 있는 이름으로 레이어 접근 가능

## 2. NN.Module 상속 방식

* **개념** : NN.Module을 상속받아 사용자 정의 모델 클래스 생성
* **특징** :
* 유연하고 복잡한 구조 구현 가능
* 코드가 상대적으로 복잡함
* 대부분의 실무 모델에 사용
* **구현 방법** :
* `__init__`에서 레이어 부품 정의
* `forward`에서 부품들을 조립하여 데이터 흐름 정의
* 복잡한 구조(ResNet, Inception 등) 구현 가능

## 3. 함수형 접근 방식

* **개념** : 파라미터가 없는 연산(ReLU, Pooling 등)은 함수형으로 사용
* **특징** :
* `torch.nn.functional`(보통 F로 임포트) 사용
* 코드가 간결해짐
* 성능 차이는 거의 없음
* **사용 예** :
  **python**copy.label

<pre><div class="text-03-R scrollbar"><span class="token keyword">import</span><span class="token plain"> torch</span><span class="token punctuation">.</span><span class="token plain">nn</span><span class="token punctuation">.</span><span class="token plain">functional </span><span class="token keyword">as</span><span class="token plain"> F</span></div><div class="text-03-R scrollbar"><span class="token plain">
  </span></div><div class="text-03-R scrollbar"><span class="token plain"></span><span class="token comment"># forward 메서드 내에서</span><span class="token plain"></span></div><div class="text-03-R scrollbar"><span class="token plain">x </span><span class="token operator">=</span><span class="token plain"> F</span><span class="token punctuation">.</span><span class="token plain">relu</span><span class="token punctuation">(</span><span class="token plain">self</span><span class="token punctuation">.</span><span class="token plain">linear1</span><span class="token punctuation">(</span><span class="token plain">x</span><span class="token punctuation">)</span><span class="token punctuation">)</span></div></pre>

## 중요 포인트

* **배치 크기(Batch Size)의 의미** :
* 한 번에 처리하는 데이터 수
* 작은 배치: 더 많은 업데이트, 안정적 학습
* 큰 배치: 더 빠른 학습, 메모리 효율
* **활성화 함수의 중요성** :
* 선형 레이어만 쌓으면 단순 스칼라 곱셈만 발생
* 활성화 함수가 비선형성 추가 → 복잡한 패턴 학습 가능
* 출력층에는 활성화 함수 적용하지 않음
* **하이브리드 접근법** :
* 실제 프로젝트에서는 여러 방식을 조합하여 사용
* 반복되는 구조는 Sequential로 정의한 후 Module 내에서 조합

## 실무 응용

* 신경망 구축시 입출력 형태 명확히 정의
* 배치 차원 고려하기 (예: [64, 784] 입력 → [64, 10] 출력)
* 모델 구조와 목적에 맞는 구축 방법 선택
* 복잡한 모델은 단계적으로 구축 및 테스트

#### 부정강조


**SCRIPT 내 주의사항/하지 말아야 할 점**

1. **복잡한 신경망(분기, 병렬, 스킵 연결 등)에는 NN.Sequential을 사용하지 마세요.**
   * 예: Transformer, ResNet, Inception 등은 Sequential로 구현 불가
2. **입력(input)과 출력(output) 구조를 먼저 명확히 결정하지 않으면 신경망이 의미 없거나 에러가 납니다.**
   * 중간 레이어 쌓기보다 입출력 설계가 우선
3. **출력층에는 활성화 함수(ReLU 등)를 사용하지 마세요.**
   * 활성화 함수는 반드시 중간(히든) 레이어에만 적용
   * 출력값이 왜곡될 수 있음
4. **배치 사이즈는 무조건 크게 또는 작게만 설정하지 마세요.**
   * 하드웨어 환경과 학습 목적에 맞게 신중하게 선택
5. **중간 구조(히든 레이어 등)만 신경 쓰고 입력/출력 구조를 무시하지 마세요.**
   * 신경망의 핵심은 입출력 구조임
6. **파라미터(가중치) 개수, 입력/출력 shape 등은 반드시 직접 확인하세요.**
   * 자동으로 맞춰질 것이라고 방심하지 말 것

## [오후수업]


## 파이토치 신경망 모델 구축 강의 요약

## 강의 개요

이 강의는 파이토치(PyTorch)를 사용하여 신경망 모델을 구축하는 방법에 대해 다룹니다. nn.Module 상속을 활용한 신경망 모델 생성, Functional 방식 활용, 모듈리스트를 이용한 동적 모델 구성, 다양한 레이어 종류와 활성화 함수, 그리고 손실 함수에 대한 내용을 포함합니다.

## 핵심개념 정리

### 1. 신경망 모델 구축 방법

* **3가지 모델 구축 방법** :

1. Sequential 방식: 레이어를 순차적으로 쌓는 방식
2. nn.Module 상속 방식: 클래스를 정의하고 모듈을 상속받아 구현
3. 모듈리스트(ModuleList) 활용: 동적이고 반복적인 구조 구현

* **nn.Module 상속 구현 핵심 요소** :
* `__init__`: 부품(레이어) 정의
* `forward`: 부품 조립(데이터 흐름 정의)
* 반드시 `super().__init__()` 호출 필요 (파라미터 추적을 위해)

### 2. Functional 방식 활용

* nn 모듈과 functional 방식 비교:
  **python**copy.label

  <pre><div class="text-03-R scrollbar"><span class="token comment"># nn 모듈 방식</span><span class="token plain"></span></div><div class="text-03-R scrollbar"><span class="token plain">self</span><span class="token punctuation">.</span><span class="token plain">relu1 </span><span class="token operator">=</span><span class="token plain"> nn</span><span class="token punctuation">.</span><span class="token plain">ReLU</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token plain"></span></div><div class="text-03-R scrollbar"><span class="token plain">x </span><span class="token operator">=</span><span class="token plain"> self</span><span class="token punctuation">.</span><span class="token plain">relu1</span><span class="token punctuation">(</span><span class="token plain">x</span><span class="token punctuation">)</span><span class="token plain"></span></div><div class="text-03-R scrollbar"><span class="token plain">
  </span></div><div class="text-03-R scrollbar"><span class="token plain"></span><span class="token comment"># functional 방식</span><span class="token plain"></span></div><div class="text-03-R scrollbar"><span class="token plain"></span><span class="token keyword">import</span><span class="token plain"> torch</span><span class="token punctuation">.</span><span class="token plain">nn</span><span class="token punctuation">.</span><span class="token plain">functional </span><span class="token keyword">as</span><span class="token plain"> F</span></div><div class="text-03-R scrollbar"><span class="token plain">x </span><span class="token operator">=</span><span class="token plain"> F</span><span class="token punctuation">.</span><span class="token plain">relu</span><span class="token punctuation">(</span><span class="token plain">x</span><span class="token punctuation">)</span></div></pre>
* functional 방식의 장점: 코드 간결화, 부품 선언 없이 바로 사용 가능

### 3. 모듈리스트(ModuleList) 활용

* 동일한 구조의 레이어를 반복적으로 사용할 때 유용
* 트랜스포머 구조 등에서 많이 활용됨
* 일반 리스트와 달리 파라미터 추적 가능
* 구현 예:
  **python**copy.label

  <pre><div class="text-03-R scrollbar"><span class="token plain">self</span><span class="token punctuation">.</span><span class="token plain">hidden_layers </span><span class="token operator">=</span><span class="token plain"> nn</span><span class="token punctuation">.</span><span class="token plain">ModuleList</span><span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token plain"></span></div><div class="text-03-R scrollbar"><span class="token plain">    nn</span><span class="token punctuation">.</span><span class="token plain">Linear</span><span class="token punctuation">(</span><span class="token plain">hidden_size</span><span class="token punctuation">,</span><span class="token plain"> hidden_size</span><span class="token punctuation">)</span><span class="token plain"></span><span class="token keyword">for</span><span class="token plain"> _ </span><span class="token keyword">in</span><span class="token plain"></span><span class="token builtin">range</span><span class="token punctuation">(</span><span class="token plain">num_hidden_layers</span><span class="token punctuation">)</span><span class="token plain"></span></div><div class="text-03-R scrollbar"><span class="token plain"></span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token plain"></span></div><div class="text-03-R scrollbar"><span class="token plain">
  </span></div><div class="text-03-R scrollbar"><span class="token plain"></span><span class="token comment"># forward에서 사용</span><span class="token plain"></span></div><div class="text-03-R scrollbar"><span class="token plain"></span><span class="token keyword">for</span><span class="token plain"> layer </span><span class="token keyword">in</span><span class="token plain"> self</span><span class="token punctuation">.</span><span class="token plain">hidden_layers</span><span class="token punctuation">:</span><span class="token plain"></span></div><div class="text-03-R scrollbar"><span class="token plain">    x </span><span class="token operator">=</span><span class="token plain"> F</span><span class="token punctuation">.</span><span class="token plain">relu</span><span class="token punctuation">(</span><span class="token plain">layer</span><span class="token punctuation">(</span><span class="token plain">x</span><span class="token punctuation">)</span><span class="token punctuation">)</span></div></pre>

### 4. 주요 레이어 타입

* **Linear Layer (Fully Connected/Dense Layer)** :
* 입력 데이터에 선형 변환 수행 (내적 + 편향 덧셈)
* `nn.Linear(in_features, out_features)`
* **Convolutional Layer (합성곱 레이어)** :
* 이미지 같은 그리드 형태 데이터의 공간적 특징 추출
* `nn.Conv2D(in_channels, out_channels, kernel_size, stride, padding)`
* 주요 매개변수:
  * kernel_size: 필터 크기 (보통 3x3, 5x5)
  * stride: 필터 이동 간격
  * padding: 입력 이미지 테두리 확장
* **Pooling Layer** :
* MaxPooling: 특정 영역의 최대값을 추출
* `nn.MaxPool2d(kernel_size, stride)`
* 목적: 차원 축소, 중요 특징 유지, 위치 변동에 덜 민감하게 함
* **기타 유용한 레이어** :
* Dropout: 과적합 방지를 위해 일부 뉴런을 무작위로 비활성화
* BatchNormalization: 학습 안정화와 가속화를 위한 정규화
* Flatten: 다차원 배열을 1차원 벡터로 변환

### 5. 활성화 함수

* **ReLU (Rectified Linear Unit)** :
* `f(x) = max(0, x)`: 양수는 그대로, 음수는 0으로
* 장점: 기울기 소실 문제 완화, 빠른 계산
* 중간 레이어에서 주로 사용
* **Sigmoid** :
* `f(x) = 1/(1+e^(-x))`: 0~1 사이의 값으로 변환
* 이진 분류의 출력층에서 사용
* 문제점: 기울기 소실(vanishing gradient) 발생 가능
* **Hyperbolic Tangent (tanh)** :
* -1~1 사이의 값으로 변환
* Sigmoid보다 범위가 넓지만 여전히 기울기 소실 가능
* **Softmax** :
* 다중 클래스 분류에서 출력층에 사용
* 여러 클래스에 대한 확률 분포 생성

### 6. 손실 함수

* **회귀 문제용** :
* MSE (Mean Squared Error): 예측값과 실제값 차이의 제곱 평균
* L1 Loss (MAE): 예측값과 실제값 차이의 절대값 평균
* **분류 문제용** :
* CrossEntropyLoss: 다중 분류 문제에서 사용
* BCELoss (Binary Cross Entropy): 이진 분류 문제에서 사용
* **주의사항** : 파이토치의 `nn.CrossEntropyLoss`는 내부적으로 softmax를 포함하므로 모델 출력에 별도의 softmax를 적용하지 않아야 함

### 7. 잔차 블록(Residual Block)

* 입력 x를 여러 레이어에 통과시킨 결과(F(x))에 원래 입력값(x)을 더하는 구조
* 기울기 소실 문제를 완화하고 더 깊은 네트워크 학습 가능
* 구현: `out = F(x) + x`

## 강조사항

1. 신경망에서 활성화 함수(ReLU 등)는 비선형성을 도입하는 핵심 역할을 함. 활성화 함수가 없으면 여러 선형 레이어도 하나의 선형 변환과 동일한 효과만 있음.
2. BatchNormalization의 중요성:
   * 학습 안정화
   * 기울기 소실 방지
   * 일반적으로 컨볼루션 레이어와 활성화 함수 사이에 위치
3. 손실 함수 선택 시 주의사항:
   * 회귀 문제: MSE 사용
   * 다중 분류: CrossEntropyLoss 사용
   * 이진 분류: BCELoss 사용
4. 모듈 상속 시 항상 `super().__init__()`를 호출해야 파라미터 추적이 가능함

## 부정강조


* **신경망 중간층에 sigmoid 쓰지 마세요.**
  → sigmoid는 중간층에 쓰면 기울기 소실(베이싱 그래디언트) 문제가 발생합니다.
* **파이토치에서 cross entropy loss 쓸 때 softmax 따로 쓰지 마세요.**
  → cross entropy는 내부적으로 softmax를 포함하므로 중복 적용하면 결과가 왜곡됩니다.
* **파이썬 리스트로 레이어 관리하지 마세요.**
  → nn.ModuleList를 사용해야 파라미터 추적 및 디바이스 이동이 제대로 됩니다.
* **회귀 문제에 cross entropy 쓰지 마세요.**
  → 회귀(연속값 예측)에는 MSELoss 등 회귀용 손실함수를 써야 합니다.
* **분류 문제에 MSE 쓰지 마세요.**
  → 분류(클래스 예측)에는 cross entropy 등 분류용 손실함수를 써야 합니다.
* **클래스는 한 셀에 하나만 만드세요.**
  → 여러 클래스를 한 셀에 만들면 코드 관리와 실행에 혼란이 생길 수 있습니다.
* .
