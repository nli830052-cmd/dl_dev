{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1f5b89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.1+cu121\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "\n",
    "# cu가 안뜨면 CPU버전이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f07e1fc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.10.12 (main, Nov  4 2025, 08:48:33) [GCC 11.4.0]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5fd2c6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA GeForce GTX 1660 SUPER\n"
     ]
    }
   ],
   "source": [
    "# CUDA(GPU) 사용 가능 여부 확인 -->주석자체가 프롬프트다.\n",
    "# 셀에 주석을 잘 걸어놓으면 좋은 코드가 나온다.\n",
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "# GPU 이름 확인\n",
    "print(torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "60da2059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "사용가능한 GPU 수 : 1\n",
      "현재 사용 중인 GPU 인덱스 : 0\n",
      "Current GPU Name: NVIDIA GeForce GTX 1660 SUPER\n",
      "Current GPU Memory: 6.441992192 GB\n",
      "\\CUDA version: 12.1\n"
     ]
    }
   ],
   "source": [
    "# 3. GPU가 사용 가능한 경우에만, 관련된 상세 정보 출력\n",
    "# 코드 기억해야함\n",
    "if torch.cuda.is_available():\n",
    "    print(\"사용가능한 GPU 수 :\", torch.cuda.device_count()) # 현장가면 엄청 많음\n",
    "    current_gpu_index = torch.cuda.current_device()\n",
    "    print(\"현재 사용 중인 GPU 인덱스 :\", current_gpu_index) # GPU가 여러개일땐 인덱스 변경 가능, 분산처리도 가능\n",
    "    print('Current GPU Name:', torch.cuda.get_device_name(current_gpu_index))\n",
    "    print('Current GPU Memory:', torch.cuda.get_device_properties(current_gpu_index).total_memory / 1e9, \"GB\")\n",
    "    print(f'\\CUDA version: {torch.version.cuda}')\n",
    "\n",
    "else:\n",
    "    print(\"GPU가 사용 불가능합니다.\")\n",
    "\n",
    "#GPU의 메모리 사용량 확인\n",
    "#중요한건 :AI를 제어하는 개발자가 되야함, 그래야 좋은 개발자임,\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e396259b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3])\n",
      "Type of x: torch.LongTensor\n",
      "Type of x: torch.int64\n",
      "Size of x: torch.Size([3])\n",
      "Device of x: cpu\n"
     ]
    }
   ],
   "source": [
    "#4.간단한 텐서(Tensor) 생성 및 출력 \n",
    "import torch    \n",
    "x=torch.tensor([1,2,3]) # x는 변수: tensor라는 opject를 가지는 변수이다.\n",
    "                    \n",
    "print(x)\n",
    "print(f'Type of x: {x.type()}')\n",
    "# print(f'Type of x: {x.dtype()}' --> 에러발생함\n",
    "print(f'Type of x: {x.dtype}') \n",
    "print(f'Size of x: {x.size()}')\n",
    "\n",
    "print(f'Device of x: {x.device}')\n",
    "\n",
    "# 데이터가 일부는 cpu,일부는 gpu에 저장하면 학습할때 멈출수 있어서 위에 코드 필요함"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Docker Venv)",
   "language": "python",
   "name": "docker_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
