## [오전강의]

### 🤔 단서 영역

- PyTorch란 무엇인가?
- 텐서(Tensor)란 무엇이고 왜 필요한가?
- CPU와 GPU 사이의 데이터 이동은 어떻게 이루어지는가?
- 텐서의 차원별 이해와 용도는 무엇인가?
- 신경망 학습에서 포워드 러닝과 백워드 러닝은 어떤 차이가 있는가?
- 자동 미분이란 무엇이며 왜 중요한가?

### 📝 필기 영역

### 텐서 저장(생성/이동) 4가지 방법 비교

### 🎯 주제

* PyTorch 개발 환경 설정 및 텐서(Tensor) 기초 이해하기
* GPU와 CPU 간의 데이터 이동 및 연산 처리 방식 학습
* 텐서의 개념과 신경망에서의 역할 이해하기

### 📝 핵심개념 정리

1. **PyTorch와 CUDA 환경 설정**
   * PyTorch는 신경망 구축을 위한 엔진으로, 현재 2.3.1 버전(CU121)을 사용
   * CUDA 사용 가능 여부 확인 방법: `torch.cuda.is_available()` 함수 사용
   * GPU 사용 가능 시 정보 확인: `torch.cuda.device_count()`, `torch.cuda.current_device()`
   * ⭐ GPU 사용 여부 확인 코드는 항상 필요하며, 기억해두어야 함
2. **텐서(Tensor)의 개념과 중요성**
   * ⭐ 텐서는 신경망이 받아들일 수 있는 데이터 형태 (신경망의 재료)
   * 텐서는 다차원 배열로, 동일 타입의 데이터만 저장하는 저장 공간
   * numpy의 ndarray와 유사하지만 동일한 개념은 아님 - PyTorch에 최적화됨
   * 텐서는 파이토치의 심장으로, 모든 딥러닝 모델의 기본 구성요소
3. **텐서의 생성 및 이동**
   * 텐서 기본 생성 방법: `torch.tensor([data])`
   * 텐서는 기본적으로 CPU에 생성됨 (device 지정 없을 시)
   * 텐서 차원에 따른 분류: 0D(스칼라), 1D(벡터), 2D(행렬), 3D 이상(텐서)
   * 데이터 타입 지정: `dtype=torch.float32` 등으로 설정 가능
4. **CPU와 GPU 간 텐서 이동**
   * CPU에서 GPU로 텐서 이동: `.to(device)` 메서드 사용
   * 직접 GPU에 텐서 생성: 생성 시 `device=device` 파라미터 지
   * ⭐ GPU에서 CPU로 이동: `.cpu()` 메서드 사용
   * ⭐ 텐서 이동은 복사 방식으로, 원본은 그대로 유지됨
5. **텐서 처리의 효율성**
   * GPU에 직접 생성하는 방식이 CPU에서 이동하는 것보다 약 5배 빠름
   * 대용량 데이터 처리 시 이 차이는 더 커질 수 있음
   * 서로 다른 장치(CPU/GPU)에 있는 텐서 간에는 직접 연산 불가
6. **신경망과 텐서의 관계**
   * 신경망에는 데이터(텐서), 가중치(W), 레이블이 필요
   * 신경망 학습은 순방향 학습(Forward Learning)과 역방향 학습(Backward Learning)으로 구성
   * 순방향 학습은 내적 연산을 통해 데이터 간 관계(유사도) 학습
   * 역방향 학습(오차 역전파)은 미분을 통해 오차가 최소가 되는 W를 찾는 과정
7. **PyTorch의 핵심 기능**
   * GPU 가속: 대규모 행렬 연산을 병렬 처리하여 CPU보다 훨씬 빠른 연산
   * 자동 미분(Autograd): 역전파에 필요한 미분을 자동으로 처리
   * ⭐ 이 두 기능으로 인해 PyTorch가 널리 사용되게 됨
8. **차원별 텐서와 데이터 유형**
   * 흑백 이미지: 3차원 텐서(높이, 폭, 장수)
   * 컬러 이미지: 4차원 텐서(높이, 폭, RGB, 장수)
   * 동영상: 5차원 텐서(높이, 폭, RGB, 장수, 시간)

### ✍️ 1.텐서생성방식 4가지

| 방법 번호 | 설명                  | 코드 예시                                | 저장 위치         | 특징 및 주의점                   |
| --------- | --------------------- | ---------------------------------------- | ----------------- | -------------------------------- |
| 1         | CPU에 직접 생성       | `torch.tensor([1,2,3])`                | RAM (CPU 메모리)  | 기본 동작, device 미지정 시 자동 |
| 2         | CPU→GPU로 복사(이동) | `tensor.to('cuda')`                    | VRAM (GPU 메모리) | 복사본 생성, 원본은 CPU에 남음   |
| 3         | GPU에 직접 생성       | `torch.tensor([1,2,3], device='cuda')` | VRAM (GPU 메모리) | 효율적, 복사 시간 없음           |
| 4         | GPU→CPU로 복사(이동) | `tensor_on_gpu.cpu()`                  | RAM (CPU 메모리)  | 복사본 생성, 원본은 GPU에 남음   |

### 2. 텐서와 numpy ndarray 비교

| 항목             | PyTorch Tensor         | numpy ndarray            |
| ---------------- | ---------------------- | ------------------------ |
| 신경망 입력 가능 | O                      | X                        |
| GPU 연산 지원    | O                      | X                        |
| 데이터 타입      | 동일 타입만 저장       | 동일 타입만 저장         |
| 생성/변환        | 다양한 함수 제공       | 다양한 함수 제공         |
| 호환성           | numpy와 상호 변환 가능 | PyTorch와 상호 변환 가능 |

---

### 3. 텐서의 dtype 지정 방식

| 지정 방식  | 자동 추론                 | 명시적 지정                                    |
| ---------- | ------------------------- | ---------------------------------------------- |
| 코드 예시  | `torch.tensor([1,2,3])` | `torch.tensor([1,2,3], dtype=torch.float32)` |
| 결과 dtype | 값에 따라 자동 결정       | 지정한 dtype                                   |
| 장점       | 간편함                    | 일관성, 오류 방지                              |
| 단점       | 의도치 않은 타입 생성     | 코드가 길어짐<br />                            |

### 4.부정강조

1. **텐서와 numpy ndarray 혼동 금지**
   * 텐서는 numpy의 ndarray와 호환성은 좋지만, "ndarray는 아니다". 텐서는 토치 전용이다. (11:34, 12:32, 35:52)
   * numpy의 ndarray를 신경망에 바로 넣으면 안 됩니다. 반드시 텐서로 변환해야 합니다. (35:52)
2. **데이터 타입 자동 추론 관련**
   * 텐서 생성 시 값에 따라 dtype이 자동 결정되지만, "맡겨놓으시면 안 됩니다." 명시적으로 dtype을 지정하는 것이 좋습니다. (35:44)
3. **텐서 이동 관련**
   * CPU→GPU, GPU→CPU 이동은 "이동"이 아니라 "복사"입니다. 원본이 남으니 혼동하지 마세요. (50:11, 53:06)
   * 큰 데이터 이동 시 메모리 부족에 주의하세요. 필요시 원본을 삭제해야 할 수도 있습니다. (54:05)
4. **연산 장치 혼합 사용 금지**
   * 서로 다른 장치(CPU, GPU)에 있는 텐서끼리는 직접 연산할 수 없습니다. 반드시 한쪽으로 이동시켜야 합니다. (01:06:39)
5. **GPU/CPU 역할 혼동 금지**
   * GPU는 행렬 연산 전용, 프린트 등 입출력은 CPU가 담당합니다. GPU가 직접 화면 출력/키보드 입력을 제어할 수 없습니다. (18:24, 19:20)
6. **메모리 용량 관련**
   * GPU(VRAM) 용량이 부족하면 에러가 발생하니, 장비의 메모리 크기를 반드시 확인하세요. (59:57)
7. **import 순서**
   * 파이썬 내장 모듈을 먼저 import하고, 외장 모듈(예: torch)은 나중에 import하세요. (22:15)


## [오후강의]

### 🎯 주제

텐서 다루기: PyTorch 텐서의 생성, 속성 및 데이터 처리 방법

### 📝 핵심개념 정리

#### 1. 텐서 생성 방법

1. **리스트로부터 텐서 생성**
   * 0차원 텐서(스칼라): `torch.tensor(7)` - 단일 값을 가진 텐서
   * 1차원 텐서(벡터): `torch.tensor([1, 2, 3, 4])` - 리스트를 텐서로 변환
   * 2차원 텐서(행렬): `torch.tensor([[1, 2, 3], [4, 5, 6]])` - 2차원 리스트를 텐서로 변환
   * 데이터 타입 자동 추론: 서로 다른 데이터 타입이 있을 때는 더 큰 범위의 타입으로 자동 변환
   * 문자열은 텐서로 변환 불가능
2. **NumPy 배열과 텐서 변환**
   * NumPy 배열에서 텐서 생성: `torch.tensor(numpy_array)` - 데이터 복사 방식
   * `torch.from_numpy(numpy_array)` - 메모리 공유 방식
   * ⭐ NumPy 배열과 텐서 간 변환 시 메모리 공유 여부 확인 방법: `.data_ptr()`로 메모리 주소 확인
3. **직접 데이터로부터 텐서 생성**
   * 지정된 데이터 타입 및 디바이스로 텐서 생성
   * GPU 텐서 생성: `device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')`
   * 불린 텐서: `torch.tensor([True, False, True])`, 타입은 `torch.bool`
4. **특정 크기와 값으로 텐서 생성**
   * `torch.zeros(3, 4)` - 모든 원소가 0인 텐서 생성 (기본 dtype은 float32)
   * `torch.ones(2, 2, 3)` - 모든 원소가 1인 텐서 생성
   * `torch.rand(size)` - 0과 1 사이의 균등분포에서 랜덤 값 생성
   * `torch.randn(size)` - 평균 0, 표준편차 1인 정규분포에서 랜덤 값 생성
   * `torch.randint(low, high, size)` - 지정 범위의 정수 랜덤 생성
5. **시퀀스 값 생성**
   * `torch.arange(start, end, step)` - 지정된 간격으로 증가하는 정수 시퀀스
   * `torch.linspace(start, end, steps)` - 구간을 균등하게 분할한 값(끝점 포함)
6. **다른 텐서의 속성을 복사하여 생성**
   * `torch.zeros_like(tensor)` - 동일한 크기의 0으로 채워진 텐서
   * `torch.ones_like(tensor)` - 동일한 크기의 1로 채워진 텐서
   * `torch.rand_like(tensor)` - 동일한 크기의 랜덤 값으로 채워진 텐서
   * ⭐ 모델 유지보수에 용이: 원본 텐서 속성이 바뀌어도 코드 수정 불필요

#### 2. 텐서 속성 이해

1. **기본 속성**
   * `shape` 또는 `size()`: 텐서의 차원 크기 확인
   * `dtype`: 텐서의 데이터 타입
   * `device`: 텐서가 저장된 장치(CPU/GPU)
   * `ndim`: 텐서의 차원 수
2. **그래디언트 관련 속성**
   * `requires_grad`: 텐서에 대한 기울기(미분) 계산 여부 설정
   * ⭐ 신경망에서 학습 가능한 파라미터 제어에 중요: 미세조정 시 일부 파라미터만 학습 가능하게 설정
   * `grad`: 계산된 기울기 저장 속성, 학습 과정 모니터링에 활용
   * `grad_fn`: 텐서를 생성한 연산 함수 참조
   * `is_leaf`: 계산 그래프의 입력 노드(끝 노드) 여부

#### 3. 데이터 분포 이해

1. **균등분포(Uniform Distribution)**
   * `torch.rand()` 함수로 생성, 0과 1 사이의 값이 균등하게 발생
   * 특징: 모든 값이 나올 확률이 동일
2. **정규분포(Normal Distribution)**
   * `torch.randn()` 함수로 생성, 평균 0, 표준편차 1인 종 모양 분포
   * 표준정규분포 특성: 중앙값(0) 주변에 데이터가 집중

#### 4. 실무 활용 예제

1. **마스킹(Masking) 처리**

   * 이미지 처리: 특정 영역만 선택하여 처리하는 기법
   * 불린 텐서를 이용한 인덱싱: `mask[100:150, 100:150] = True`
   * 트랜스포머 모델에서도 Masked Multi-Head Attention에 활용
2. **재현성 확보**

   * `torch.manual_seed(42)` - 랜덤 값 고정으로 실험 재현성 확보
   * 토치에서는 시드를 설정한 직후의 랜덤 생성에만 적용됨
3. **GPU와 CPU 간 데이터 관리**

   * 대용량 데이터 처리 시 메모리 효율성 고려 필요
   * `torch.from_numpy()` - 메모리 복사 없이 NumPy 배열을 CPU 텐서로 변환
   * ⭐ 대용량 모델 처리: CPU 메모리에 데이터 저장 후 필요한 부분만 GPU로 전송

### **부정강조**


**텐서와 numpy ndarray 혼동 금지**

* 텐서는 numpy의 ndarray와 호환성은 좋지만, "ndarray는 아니다". 텐서는 토치 전용이다. (11:34, 12:32, 35:52)
* numpy의 ndarray를 신경망에 바로 넣으면 안 됩니다. 반드시 텐서로 변환해야 합니다. (35:52)

**데이터 타입 자동 추론 관련**

* 텐서 생성 시 값에 따라 dtype이 자동 결정되지만, "맡겨놓으시면 안 됩니다." 명시적으로 dtype을 지정하는 것이 좋습니다. (35:44)

**텐서 이동 관련**

* CPU→GPU, GPU→CPU 이동은 "이동"이 아니라 "복사"입니다. 원본이 남으니 혼동하지 마세요. (50:11, 53:06)
* 큰 데이터 이동 시 메모리 부족에 주의하세요. 필요시 원본을 삭제해야 할 수도 있습니다. (54:05)

**연산 장치 혼합 사용 금지**

* 서로 다른 장치(CPU, GPU)에 있는 텐서끼리는 직접 연산할 수 없습니다. 반드시 같은 장치로 옮긴 후 연산해야 합니다. (02:59:41)

**문자열/혼합 타입 텐서 생성 금지**

* 리스트에 문자열이 섞이면 텐서 생성이 불가능합니다. "문자열은 안 됩니다."
* 데이터 타입이 혼합된 리스트(특히 문자 포함)는 에러가 발생하니 주의하세요.

**numpy 2.0 버전 사용 금지**

* numpy 2.0 버전에서는 nan이 Nan으로 바뀌어 기존 코드가 에러가 납니다. "2.0 버전은 쓰지 마세요."

**requires_grad 설정 실수 주의**

* 미분/학습이 필요 없는 텐서에 true를 주지 말고, 필요한 텐서에 false를 주지 마세요. (03:09:26)

**파이썬 변수 타입 명시 습관화**

* 타입 명시를 하지 않으면 나중에 변수 타입을 혼동해서 에러가 날 수 있습니다. 습관적으로 타입을 명시하세요.

**프로젝트 네이밍 룰 무시 금지**

* 변수명, 디렉토리명 등은 반드시 프로젝트의 네이밍 규칙을 확인하고, 마음대로 정하지 마세요.

**AI 자동완성(엔트) 맹신 금지**

* AI 자동완성 기능에만 의존하지 말고, 직접 코딩 연습을 충분히 하세요.
