{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c06aed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/1000], Loss: 9.4269, w: 1.9921, b: -1.2428\n",
      "Epoch [200/1000], Loss: 7.4283, w: 1.9933, b: -0.7971\n",
      "Epoch [300/1000], Loss: 6.0887, w: 1.9943, b: -0.4323\n",
      "Epoch [400/1000], Loss: 5.1909, w: 1.9951, b: -0.1336\n",
      "Epoch [500/1000], Loss: 4.5891, w: 1.9957, b: 0.1109\n",
      "Epoch [600/1000], Loss: 4.1858, w: 1.9962, b: 0.3111\n",
      "Epoch [700/1000], Loss: 3.9155, w: 1.9967, b: 0.4750\n",
      "Epoch [800/1000], Loss: 3.7343, w: 1.9970, b: 0.6092\n",
      "Epoch [900/1000], Loss: 3.6129, w: 1.9973, b: 0.7190\n",
      "Epoch [1000/1000], Loss: 3.5315, w: 1.9976, b: 0.8090\n",
      "\n",
      "Training finished. Learned parameters: w = 1.9976, b = 0.8090\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "# 1. 데이터 준비\n",
    "# y = 2 * x + 1 에 노이즈를 추가한 가상 데이터\n",
    "X = torch.randn(100, 1) * 10\n",
    "y = 2 * X + 1 + torch.randn(100, 1) * 2\n",
    "\n",
    "\n",
    "# 2. 모델 파라미터 초기화\n",
    "w = torch.randn(1, requires_grad=False) # autograd를 사용하지 않으므로 False\n",
    "b = torch.randn(1, requires_grad=False)\n",
    "\n",
    "\n",
    "# 3. 하이퍼파라미터 설정\n",
    "learning_rate = 1e-3 \n",
    "epochs = 1000# 반복횟수 중요함\n",
    "\n",
    "\n",
    "# 4. 훈련 루프\n",
    "for epoch in range(epochs):\n",
    "    # 순전파: 예측값 계산\n",
    "    y_pred = w * X + b\n",
    "   \n",
    "    # 손실 계산 (MSE)\n",
    "    loss = torch.mean((y_pred - y) ** 2)\n",
    "   \n",
    "    # 역전파: 기울기 수동 계산\n",
    "    # d(loss)/d(w) 와 d(loss)/d(b)\n",
    "    grad_w = torch.mean(2 * (y_pred - y) * X)\n",
    "    grad_b = torch.mean(2 * (y_pred - y))\n",
    "   \n",
    "    # 파라미터 업데이트\n",
    "    w = w - learning_rate * grad_w\n",
    "    b = b - learning_rate * grad_b\n",
    "   \n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}, w: {w.item():.4f}, b: {b.item():.4f}')\n",
    "\n",
    "\n",
    "print(f'\\nTraining finished. Learned parameters: w = {w.item():.4f}, b = {b.item():.4f}')\n",
    "\n",
    "# Loss가 줄지만, 줄어드는건 중요한게 아니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d5ffa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [200/2000], Loss: 235.2242\n",
      "Epoch [400/2000], Loss: 108.3784\n",
      "Epoch [600/2000], Loss: 52.6331\n",
      "Epoch [800/2000], Loss: 28.1171\n",
      "Epoch [1000/2000], Loss: 17.3269\n",
      "Epoch [1200/2000], Loss: 12.5736\n",
      "Epoch [1400/2000], Loss: 10.4774\n",
      "Epoch [1600/2000], Loss: 9.5520\n",
      "Epoch [1800/2000], Loss: 9.1429\n",
      "Epoch [2000/2000], Loss: 8.9618\n",
      "\n",
      "Result: y = 0.0044 + 0.8458x + -0.0008x^2 + -0.0918x^3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "\n",
    "# 1. 데이터 준비\n",
    "dtype = torch.float\n",
    "device = \"cpu\"\n",
    "x = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype)\n",
    "y = torch.sin(x)\n",
    "\n",
    "\n",
    "# 2. 모델 파라미터(가중치) 랜덤 초기화\n",
    "a = torch.randn((), device=device, dtype=dtype)\n",
    "b = torch.randn((), device=device, dtype=dtype)\n",
    "c = torch.randn((), device=device, dtype=dtype)\n",
    "d = torch.randn((), device=device, dtype=dtype)\n",
    "\n",
    "\n",
    "# 3. 하이퍼파라미터 설정\n",
    "learning_rate = 1e-6\n",
    "epochs = 2000\n",
    "\n",
    "\n",
    "# 4. 훈련 루프\n",
    "for t in range(epochs):\n",
    "    # 순전파: 예측값 y_pred 계산\n",
    "    y_pred = a + b * x + c * x ** 2 + d * x ** 3\n",
    "\n",
    "\n",
    "    # 손실(Loss) 계산: Mean Squared Error (MSE)\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    if t % 200 == 199:\n",
    "        print(f'Epoch [{t+1}/{epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "\n",
    "    # 역전파: 손으로 직접 기울기 계산\n",
    "    # loss = sum((y_pred - y)^2)\n",
    "    # d(loss)/d(param) = d(loss)/d(y_pred) * d(y_pred)/d(param)\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_a = grad_y_pred.sum()\n",
    "    grad_b = (grad_y_pred * x).sum()\n",
    "    grad_c = (grad_y_pred * x ** 2).sum()\n",
    "    grad_d = (grad_y_pred * x ** 3).sum()\n",
    "\n",
    "\n",
    "    # 경사하강법으로 파라미터 업데이트\n",
    "    a -= learning_rate * grad_a\n",
    "    b -= learning_rate * grad_b\n",
    "    c -= learning_rate * grad_c\n",
    "    d -= learning_rate * grad_d\n",
    "\n",
    "\n",
    "print(f'\\nResult: y = {a.item():.4f} + {b.item():.4f}x + {c.item():.4f}x^2 + {d.item():.4f}x^3')\n",
    "\n",
    "\n",
    "# 오차가 줄고 있다  즉 학습이 일어나고 있다\n",
    "#  어느순간 오차가 줄지 않거나 오히려 오차가 늘어나는 경우도 생길 수 있다.\n",
    "# 어쨌든 w 찾기가 핵심이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4665ecb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Leaf Node 'a' ---\n",
      "a.data: 2.0\n",
      "a.requires_grad: True\n",
      "a.grad: None\n",
      "a.grad_fn: None\n",
      "a.is_leaf: True\n",
      "\n",
      "--- Intermediate Node 'c' ---\n",
      "c.data: 5.0\n",
      "c.requires_grad: True\n",
      "c.grad_fn: <AddBackward0 object at 0x7c117755e9e0>\n",
      "c.is_leaf: False\n",
      "\n",
      "--- Root Node 'Q' ---\n",
      "Q.data: 20.0\n",
      "Q.requires_grad: True\n",
      "Q.grad_fn: <MulBackward0 object at 0x7c117755de40>\n",
      "Q.is_leaf: False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "# 1. 잎 노드(leaf nodes) 생성. 기울기 계산이 필요하므로 requires_grad=True로 설정.\n",
    "a = torch.tensor(2.0, requires_grad=True)\n",
    "b = torch.tensor(3.0, requires_grad=True)\n",
    "\n",
    "\n",
    "# 2. 연산을 통해 중간 노드와 루트 노드 생성\n",
    "c = a + b\n",
    "Q = c * (b + 1)\n",
    "\n",
    "\n",
    "# 3. 각 텐서의 속성 확인\n",
    "print(\"--- Leaf Node 'a' ---\")\n",
    "print(f\"a.data: {a.data}\") # 텐서에서 값을꺼낼땐 .data사용\n",
    "print(f\"a.requires_grad: {a.requires_grad}\")\n",
    "print(f\"a.grad: {a.grad}\")\n",
    "print(f\"a.grad_fn: {a.grad_fn}\") # 사용자가 직접 생성했으므로 grad_fn이 없음\n",
    "print(f\"a.is_leaf: {a.is_leaf}\\n\") # 끝에 있다.\n",
    "\n",
    "\n",
    "print(\"--- Intermediate Node 'c' ---\")\n",
    "print(f\"c.data: {c.data}\")\n",
    "print(f\"c.requires_grad: {c.requires_grad}\") # requires_grad=True인 텐서로부터 파생되었으므로 True\n",
    "print(f\"c.grad_fn: {c.grad_fn}\") # 덧셈의 미분 함수를 가리킴\n",
    "print(f\"c.is_leaf: {c.is_leaf}\\n\") # 연산의 결과이므로 잎 노드가 아님\n",
    "\n",
    "\n",
    "print(\"--- Root Node 'Q' ---\")\n",
    "print(f\"Q.data: {Q.data}\")\n",
    "print(f\"Q.requires_grad: {Q.requires_grad}\")\n",
    "print(f\"Q.grad_fn: {Q.grad_fn}\") # 곱셈의 미분 함수를 가리킴\n",
    "print(f\"Q.is_leaf: {Q.is_leaf}\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "80cc37ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Gradients after Q.backward() ---\n",
      "Gradient of Q with respect to a (dQ/da): 4.0\n",
      "Gradient of Q with respect to b (dQ/db): 9.0\n",
      "Gradient of c: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2441/3179757177.py:21: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at aten/src/ATen/core/TensorBody.h:489.)\n",
      "  print(f\"Gradient of c: {c.grad}\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "a = torch.tensor(2.0, requires_grad=True)\n",
    "b = torch.tensor(3.0, requires_grad=True)\n",
    "c = a + b\n",
    "Q = c * (b + 1)\n",
    "\n",
    "\n",
    "# 역전파 수행\n",
    "Q.backward()\n",
    "\n",
    "\n",
    "# 각 잎 노드의 .grad 속성에 저장된 기울기 확인\n",
    "print(\"--- Gradients after Q.backward() ---\")\n",
    "print(f\"Gradient of Q with respect to a (dQ/da): {a.grad}\")\n",
    "print(f\"Gradient of Q with respect to b (dQ/db): {b.grad}\")\n",
    "\n",
    "\n",
    "# 중간 노드의 기울기는 기본적으로 저장되지 않음\n",
    "print(f\"Gradient of c: {c.grad}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0af91e44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [200/2000], Loss: 1338.0642\n",
      "Epoch [400/2000], Loss: 657.5425\n",
      "Epoch [600/2000], Loss: 326.2810\n",
      "Epoch [800/2000], Loss: 164.5588\n",
      "Epoch [1000/2000], Loss: 85.3935\n",
      "Epoch [1200/2000], Loss: 46.5459\n",
      "Epoch [1400/2000], Loss: 27.4402\n",
      "Epoch [1600/2000], Loss: 18.0248\n",
      "Epoch [1800/2000], Loss: 13.3764\n",
      "Epoch [2000/2000], Loss: 11.0777\n",
      "\n",
      "Result: y = -0.0486 + 0.8446x + 0.0084x^2 + -0.0916x^3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "\n",
    "# 1. 데이터 준비 (동일)\n",
    "dtype = torch.float\n",
    "device = \"cpu\"\n",
    "x = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype)\n",
    "y = torch.sin(x)\n",
    "\n",
    "\n",
    "# 2. 모델 파라미터(가중치) 랜덤 초기화\n",
    "# 핵심: requires_grad=True로 설정하여 autograd가 이 텐서들을 추적하도록 함\n",
    "a = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
    "b = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
    "c = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
    "d = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "\n",
    "# 3. 하이퍼파라미터 설정\n",
    "learning_rate = 1e-6\n",
    "epochs = 2000\n",
    "\n",
    "\n",
    "# 4. 훈련 루프\n",
    "for t in range(epochs):\n",
    "    # 순전파: 예측값 y_pred 계산 (동일)\n",
    "    y_pred = a + b * x + c * x ** 2 + d * x ** 3\n",
    "\n",
    "\n",
    "    # 손실(Loss) 계산 (동일)\n",
    "    # 이제 loss는 스칼라 값이 아닌, grad_fn을 가진 0차원 텐서가 됨\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    if t % 200 == 199:\n",
    "        print(f'Epoch [{t+1}/{epochs}], Loss: {loss.item():.4f}') # .item()으로 스칼라 값 추출\n",
    "\n",
    "\n",
    "    # 역전파: autograd를 사용하여 자동으로 기울기 계산\n",
    "    # 이 한 줄이 수동 기울기 계산 코드를 모두 대체!\n",
    "    loss.backward()\n",
    "\n",
    "\n",
    "    # 경사하강법으로 파라미터 업데이트\n",
    "    # 주의: 기울기 업데이트 연산은 추적할 필요가 없으므로 torch.no_grad() 블록 안에서 수행\n",
    "    with torch.no_grad():\n",
    "        a -= learning_rate * a.grad\n",
    "        b -= learning_rate * b.grad\n",
    "        c -= learning_rate * c.grad\n",
    "        d -= learning_rate * d.grad\n",
    "\n",
    "\n",
    "        # 중요: 다음 반복을 위해 기울기를 수동으로 0으로 초기화\n",
    "        # 그렇지 않으면 기울기가 계속 누적됨\n",
    "        a.grad.zero_()\n",
    "        b.grad.zero_()\n",
    "        c.grad.zero_()\n",
    "        d.grad.zero_()\n",
    "\n",
    "\n",
    "print(f'\\nResult: y = {a.item():.4f} + {b.item():.4f}x + {c.item():.4f}x^2 + {d.item():.4f}x^3')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee69b32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
      "100%|██████████| 44.7M/44.7M [00:01<00:00, 33.1MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Freezing all parameters ---\n",
      "\n",
      "--- Checking requires_grad status after modification ---\n",
      "  Trainable: fc.weight, Shape: torch.Size([10, 512])\n",
      "  Trainable: fc.bias, Shape: torch.Size([10])\n",
      "\n",
      "Total trainable parameters: 5130\n",
      "Total frozen parameters: 11176512\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "\n",
    "\n",
    "# 1. 사전 훈련된 resnet18 모델 로드\n",
    "model = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "\n",
    "\n",
    "# 2. 모델의 모든 파라미터를 고정\n",
    "print(\"--- Freezing all parameters ---\")\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "    # print(f\"Parameter shape: {param.shape}, requires_grad={param.requires_grad}\") # 모든 파라미터 확인용\n",
    "\n",
    "\n",
    "# 3. 마지막 분류기(classifier) 레이어만 새로운 레이어로 교체\n",
    "# 새로운 레이어의 파라미터는 기본적으로 requires_grad=True 입니다.\n",
    "num_features = model.fc.in_features\n",
    "model.fc = nn.Linear(num_features, 10) # 10개 클래스로 분류하는 새로운 레이어\n",
    "\n",
    "\n",
    "print(\"\\n--- Checking requires_grad status after modification ---\")\n",
    "# 4. 파라미터들의 requires_grad 상태 확인\n",
    "trainable_params = 0\n",
    "frozen_params = 0\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"  Trainable: {name}, Shape: {param.shape}\")\n",
    "        trainable_params += param.numel()\n",
    "    else:\n",
    "        frozen_params += param.numel()\n",
    "\n",
    "\n",
    "print(f\"\\nTotal trainable parameters: {trainable_params}\")\n",
    "print(f\"Total frozen parameters: {frozen_params}\")\n",
    "\n",
    "# 마지막 층만 학습하게 만듦\n",
    "# parameters 천만개..나중에는 더 걸림,\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ec7928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original z: tensor([-1.2030,  6.2042, -4.3672], grad_fn=<MulBackward0>)\n",
      "z.grad_fn: <MulBackward0 object at 0x7c11776af970>\n",
      "\n",
      "--- After detach() ---\n",
      "detached_z: tensor([-1.2030,  6.2042, -4.3672])\n",
      "detached_z.requires_grad: False\n",
      "detached_z.grad_fn: None\n",
      "\n",
      "--- Further operation on detached tensor ---\n",
      "w.requires_grad: False\n",
      "\n",
      "--- Memory Sharing Demonstration ---\n",
      "Original z before modification: tensor([-1.2030,  6.2042, -4.3672], grad_fn=<MulBackward0>)\n",
      "Modified detached_z: tensor([-12.0305,  62.0420, -43.6725])\n",
      "Original z after modification: tensor([-12.0305,  62.0420, -43.6725], grad_fn=<MulBackward0>)\n",
      "\n",
      "--- Backward Propagation ---\n",
      "Gradient of x (from z): tensor([6., 6., 6.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "# 기울기 추적이 필요한 텐서\n",
    "x = torch.randn(3, requires_grad=True)\n",
    "y = x * 2\n",
    "z = y * 3 \n",
    "print(f\"Original z: {z}\")\n",
    "print(f\"z.grad_fn: {z.grad_fn}\\n\")\n",
    "\n",
    "\n",
    "# 1. z를 계산 그래프에서 분리\n",
    "detached_z = z.detach()\n",
    "print(\"--- After detach() ---\")\n",
    "print(f\"detached_z: {detached_z}\")\n",
    "print(f\"detached_z.requires_grad: {detached_z.requires_grad}\")\n",
    "print(f\"detached_z.grad_fn: {detached_z.grad_fn}\\n\")\n",
    "\n",
    "\n",
    "# 2. 분리된 텐서로 추가 연산을 해도 기울기 추적이 되지 않음\n",
    "w = detached_z + 1\n",
    "print(\"--- Further operation on detached tensor ---\")\n",
    "print(f\"w.requires_grad: {w.requires_grad}\\n\")\n",
    "\n",
    "\n",
    "# 3. detach()와 메모리 공유\n",
    "# detached_z의 값을 in-place 연산으로 변경하면 원본 z의 값도 변경됨\n",
    "print(\"--- Memory Sharing Demonstration ---\")\n",
    "print(f\"Original z before modification: {z}\")\n",
    "detached_z.mul_(10) # In-place multiplication  # 원본 반영\n",
    "print(f\"Modified detached_z: {detached_z}\")\n",
    "print(f\"Original z after modification: {z}\\n\")\n",
    "\n",
    "\n",
    "# 4. 역전파 시 detach()의 효과\n",
    "# z에 대한 역전파는 x까지 정상적으로 전달됨\n",
    "z.sum().backward()\n",
    "print(\"--- Backward Propagation ---\")\n",
    "print(f\"Gradient of x (from z): {x.grad}\")\n",
    "\n",
    "\n",
    "# w.sum().backward() # 이 코드를 실행하면 detached_z가 잎 노드처럼 동작하므로 오류가 발생하지 않지만,\n",
    "# 기울기는 x까지 전파되지 않습니다.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Docker Venv)",
   "language": "python",
   "name": "docker_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
