{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d852221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Device name: NVIDIA GeForce GTX 1660 SUPER\n",
      "\n",
      "텐서가 생성된 장치: cuda:0\n",
      "이동 전 텐서 장치: cpu\n",
      "이동 후 텐서 장치: cuda:0\n",
      "\n",
      "CPU 연산 시간: 0.43470 초\n",
      "GPU 연산 시간: 0.03888 초\n",
      "GPU가 CPU보다 약 11.18배 빠릅니다.\n"
     ]
    }
   ],
   "source": [
    "# image만 있으면 언제든지 contina는 실행가능함\n",
    "# 항상주의\n",
    "# container up / down -> 만들고 지우고\n",
    "# container start / stop -> 실행하고 정지\n",
    "# GPU 썻을때와 안썼을때의 차이 비교 코드\n",
    "# 나중에 음성인식에서는 numpy 1.26버전 사용해야함\n",
    "\n",
    "import torch\n",
    "import time\n",
    "\n",
    "# 1. 장치 설정: 코드를 장치 독립적으로 만드는 핵심 패턴\n",
    "# torch.cuda.is_available()을 확인하여 GPU 사용 가능 여부를 판단합니다.\n",
    "# 가능하면 'cuda' 장치를, 불가능하면 'cpu' 장치를 사용하도록 설정합니다.\n",
    "# 이렇게 하면 코드가 어떤 환경에서도 유연하게 동작할 수 있습니다.\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Device name: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "\n",
    "# 2. 텐서를 특정 장치로 생성하거나 이동\n",
    "# 처음부터 GPU에 텐서 생성\n",
    "tensor_on_gpu = torch.randn(1000, 1000, device=device)\n",
    "print(f\"\\n텐서가 생성된 장치: {tensor_on_gpu.device}\")\n",
    "\n",
    "\n",
    "# CPU에서 생성 후 GPU로 이동\n",
    "tensor_on_cpu = torch.randn(1000, 1000)\n",
    "print(f\"이동 전 텐서 장치: {tensor_on_cpu.device}\")\n",
    "tensor_on_gpu_moved = tensor_on_cpu.to(device)\n",
    "print(f\"이동 후 텐서 장치: {tensor_on_gpu_moved.device}\")\n",
    "\n",
    "\n",
    "# 3. GPU 연산 속도 비교\n",
    "# 큰 행렬 곱셈 연산을 CPU와 GPU에서 각각 수행하고 시간을 측정합니다.\n",
    "size = 4096\n",
    "cpu_a = torch.randn(size, size, device='cpu')\n",
    "cpu_b = torch.randn(size, size, device='cpu')\n",
    "\n",
    "\n",
    "# CPU 연산 시간 측정\n",
    "start_time_cpu = time.time()\n",
    "result_cpu = torch.matmul(cpu_a, cpu_b)\n",
    "end_time_cpu = time.time()\n",
    "cpu_time = end_time_cpu - start_time_cpu\n",
    "print(f\"\\nCPU 연산 시간: {cpu_time:.5f} 초\")\n",
    "\n",
    "\n",
    "# GPU 연산 시간 측정\n",
    "if device.type == 'cuda':\n",
    "    gpu_a = cpu_a.to(device)\n",
    "    gpu_b = cpu_b.to(device)\n",
    "\n",
    "\n",
    "    # GPU 워밍업 (초기 커널 로딩 시간 제외)\n",
    "    _ = torch.matmul(gpu_a, gpu_b)\n",
    "   \n",
    "    # GPU 연산은 비동기적으로 처리될 수 있으므로, 정확한 시간 측정을 위해\n",
    "    # torch.cuda.synchronize()를 호출하여 GPU의 모든 연산이 끝날 때까지 기다립니다.\n",
    "    torch.cuda.synchronize()\n",
    "    start_time_gpu = time.time()\n",
    "    result_gpu = torch.matmul(gpu_a, gpu_b)\n",
    "    torch.cuda.synchronize()\n",
    "    end_time_gpu = time.time()\n",
    "    gpu_time = end_time_gpu - start_time_gpu\n",
    "    print(f\"GPU 연산 시간: {gpu_time:.5f} 초\")\n",
    "   \n",
    "    # 속도 향상률 계산\n",
    "    speedup = cpu_time / gpu_time\n",
    "    print(f\"GPU가 CPU보다 약 {speedup:.2f}배 빠릅니다.\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Docker Venv)",
   "language": "python",
   "name": "docker_env"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
