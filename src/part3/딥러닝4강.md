### [1]오전강의

### 🎯 주제

- 딥러닝의 핵심 원리: 자동 미분(Autograd)과 신경망의 학습 메커니즘
- 신경망의 본질적 개념과 학습 과정에 대한 이해

### 📝 핵심개념 정리

1. 신경망의 본질

   - ⭐ 신경망은 본질적으로 하나의 큰 함수(합성함수)이다
   - 신경망의 각 층은 함수로 구성되며, 층이 많아질수록 함수 안의 함수가 중첩되는 합성함수 형태를 갖는다
   - 신경망에서는 데이터가 배치 사이즈 단위로 행렬 형태로 입력된다
   - 머신러닝에서는 데이터가 벡터 단위로 입력되었지만, 신경망에서는 배치 사이즈만큼의 벡터가 모인 행렬 형태로 입력된다
2. 딥러닝의 학습 원리

   - ⭐ 딥러닝 모델 학습은 손실 함수의 값을 최소화하는 최적의 파라미터(W)를 찾는 과정이다
   - ⭐ 미분이 학습의 핵심 원리이다
   - 학습은 오차가 최소가 되는 W(가중치) 값을 찾는 것이다
   - 손실함수를 미분하여 가중치를 업데이트하는 방식으로 학습이 진행된다
3. 역전파(Backpropagation)의 이해

   - 역전파는 손실함수를 각 파라미터(W)로 편미분하여 기울기를 구하는 과정이다
   - 엄밀하게는 "오차 역전파"가 아니라 "손실함수의 편미분"이라고 봐야 한다
   - 역전파를 통해 각 파라미터가 어느 방향으로 얼마나 조절되어야 하는지 계산한다
   - 계산된 기울기의 반대 방향으로 파라미터를 조금씩 업데이트하는 과정을 반복한다
4. 경사하강법(Gradient Descent)

   - 신경망 학습에서는 경사하강법(GD)을 통해 손실함수의 최소값을 찾는다
   - 현재 위치에서 가장 가파른 내리막 방향(기울기)을 찾아 한 걸음씩 내려가는 방식이다
   - W = W - D (여기서 D는 미분 값)의 식으로 파라미터를 업데이트한다
   - 더 이상 파라미터 값의 변동이 없을 때 최소점에 도달했다고 판단한다
5. 자동 미분(Autograd)

   - 파이토치의 오토그레드(Autograd)는 복잡한 기울기 계산을 자동으로 수행해주는 기능이다
   - required_grad 속성을 True로 설정하면 텐서의 미분이 자동으로 계산된다
   - ⭐ 오토그레드를 사용하면 개발자는 순전파(forward pass) 설계에만 집중할 수 있다
   - 수동으로 미분 계산을 구현할 필요가 없어 딥러닝 모델 개발이 용이해졌다
6. 신경망 학습의 단계

   - 순전파(Forward Pass): 입력 데이터를 모델에 통과시켜 예측값을 얻는 단계
   - 손실 계산: 예측값과 실제 정답을 비교하여 오차(손실)를 계산
   - 역전파(Backward Pass): 손실함수를 각 파라미터로 편미분하여 기울기 계산
   - 파라미터 업데이트: 계산된 기울기를 사용해 파라미터 값 조정
   - 이 과정이 배치 사이즈만큼의 데이터에 대해 반복적으로 수행된다
7. 학습과 추론의 구분

   - 학습은 오차가 최소가 되는 W를 찾는 과정이다
   - 추론은 학습이 완료된 모델에 새 데이터를 입력하여 예측값을 얻는 과정이다
   - 학습이 끝났다는 것은 오차가 최소가 되는 W를 모두 찾았다는 의미이다
8. 중요한 하이퍼파라미터

   - 학습률(Learning Rate): 파라미터 업데이트의 크기를 조절하는 값
   - 에포크(Epoch): 전체 데이터셋을 몇 번 반복해서 학습할지 결정하는 값
   - 오차가 계속 줄어드는지 확인하는 것이 학습 진행 상황을 판단하는 핵심 지표이다

### 부정강조

* **오차 값을 미분하면 안 된다** : 오차 값(즉, 상수)을 미분하면 0이 되기 때문에 의미가 없습니다. 반드시 함수(식)를 미분해야 하며, 오차 값을 직접 미분하는 것은 잘못된 접근입니다.
* **오차 역전파를 값의 전파로 이해하면 안 된다** : 오차 값이 신경망을 따라 전파되는 것이 아니라, 함수의 미분(기울기)이 전파되는 것입니다. 오차 값이 각 층을 따라 전달된다고 생각하면 안 됩니다.
* **오차 역전파를 하나씩 뽑아 역전파 시킨다고 생각하면 안 된다** : 역전파는 오차 값을 하나씩 전파하는 구조가 아니라, 합성함수의 미분이 한 번에 계산되는 구조입니다. 오차 값을 단계별로 전달한다고 오해하지 말아야 합니다.
* **미분이 불가능한(끊어진) 함수는 쓰면 안 된다** : 신경망의 loss function 등은 반드시 미분 가능해야 하며, 그래프가 끊어진 함수는 미분이 불가능하므로 사용하면 안 됩니다.

### [2]오후강의

## 파이토치 계산 그래프와 오토그래드 이해하기

### 🎯 주제

이 강의는 파이토치(PyTorch)의 자동 미분 기능인 오토그래드(Autograd)와 계산 그래프(Computation Graph)에 대해 다룹니다. 신경망 학습의 핵심 메커니즘인 역전파(Backpropagation)가 어떻게 구현되는지, 기울기 계산과 파라미터 업데이트가 어떻게 이루어지는지 설명하고, 최종적으로 PyTorch의 nn 모듈을 활용한 신경망 구축 방법을 소개합니다.

### 📝 핵심개념 정리

1. **계산 그래프(Computation Graph)**
   * 파이토치에서 텐서 연산을 추적하여 만드는 방향성 비순환 그래프
   * 구성요소:
     * 노드(Node): 텐서를 저장하는 공간
     * 엣지(Edge): 텐서를 입력받아 새로운 텐서를 출력하는 함수(연산)
   * 파이토치는 동적 계산 그래프를 생성 (Define-by-run 방식)
     * 코드가 실행될 때마다, 심지어 반복문의 각 iteration마다 새롭게 생성됨
     * 이를 통해 유연한 모델링과 쉬운 디버깅이 가능함
2. **오토그래드(Autograd)와 자동 미분**
   * 텐서의 `requires_grad=True` 설정을 통해 자동 미분 활성화
   * 역전파(Backpropagation) 과정:
     * 손실 텐서(Loss tensor)에서 `backward()` 메소드를 호출하면 역전파가 시작됨
     * 그래프를 역방향으로 순회하며 미분의 연쇄법칙(Chain Rule)을 적용해 기울기 계산
     * 각 파라미터의 기울기는 `.grad` 속성에 저장됨
3. **기울기 계산 제어 방법**
   * **`requires_grad` 속성 변경**
     * `tensor.requires_grad=False`로 설정하면 해당 텐서의 기울기 추적을 영구적으로 비활성화
     * 주로 사전 훈련된 모델에서 일부 레이어만 미세조정(fine-tuning)하고 나머지는 고정(freeze)할 때 사용
   * **`torch.no_grad()` 컨텍스트 매니저**
     * 코드 블록 전체에 대한 기울기 계산을 일시적으로 비활성화
     * 주로 모델 평가(evaluation)나 추론(inference) 시 사용
     * 메모리 사용량을 줄이고 연산 속도를 높이는 효과
   * **`detach()` 메소드**
     * 현재 계산 그래프로부터 텐서를 영구적으로 분리
     * 원본 텐서와 데이터 메모리는 공유하지만 계산 기록은 공유하지 않음
     * 학습 중 중간 계산 결과 로깅, 적대적 생성 신경망(GAN) 학습에서 활용
4. **파라미터 업데이트 과정**
   * 파라미터 업데이트는 기울기 계산이 완료된 후 수동으로 수행
   * 업데이트 과정:
     1. `loss.backward()`로 기울기 계산
     2. `torch.no_grad()` 블록 안에서 파라미터 업데이트 수행
     3. 다음 반복을 위해 `param.grad.zero_()` 로 기울기 초기화
   * ⭐ 기울기 초기화를 하지 않으면 기울기가 누적되므로 반드시 초기화해야 함
5. **전이학습(Transfer Learning)**
   * 사전 훈련된 모델을 가져와 일부만 수정하여 새로운 목적에 활용하는 방식
   * 절차:
     1. 사전 훈련된 모델 로드
     2. 필요한 레이어만 제외하고 모든 파라미터 고정(`requires_grad=False`)
     3. 새로운 레이어로 교체하고 미세조정
6. **nn 모듈을 이용한 신경망 구축**
   * `nn.Module`을 상속받아 사용자 정의 모델 클래스 생성
   * 필수 구현 메소드:
     * `__init__()`: 부품(레이어) 생성 및 초기화
     * `forward()`: 부품들을 조립하여 신경망 구조 정의
   * 기본적인 레이어 구성:
     * `nn.Linear`: 선형 변환 수행 (y = Wx + b)
     * 레이어 생성 시 input_size와 output_size를 지정해야 함

### 부정강조

* **계산 그래프 관련**
  * 계산 그래프가 만들어졌다고 해서 실제 연산이 끝난 것이 아닙니다. 계산 그래프는 연산의 흐름만 정의한 것이며, 실제 값이 계산된 것은 아닙니다. ("Q는 20이 됐다라고 생각하시면 안 된다. 그 뜻입니다.")
* **Backward(역전파) 관련**
  * backward는 반드시 손실 함수(loss)에서 호출해야 하며, 중간 단계에서 임의로 호출하면 안 됩니다. ("loss function에서 바로 backward를 진행해야 된다. 라는 게 일단 중요한 겁니다.")
  * backward를 여러 번 호출할 때는 기울기(gradient)가 누적되므로, 반드시 매 반복마다 기울기를 0으로 초기화해야 합니다. 초기화하지 않으면 이전 기울기가 계속 남아 잘못된 결과가 나옵니다. ("이전 기울기를 뭘로 만들어 놔야 된다? 0으로 만들어 놔야 한다라는 겁니다.")
* **기울기(gradient) 관련**
  * 파라미터의 .grad 값을 0으로 초기화하지 않으면, 기울기가 누적되어 잘못된 학습이 됩니다. ("이전 기울기를 뭘로 만들어 놔야 된다? 0으로 만들어 놔야 한다라는 겁니다.")
  * .grad를 None으로 두는 것이 더 직관적일 수 있지만, PyTorch에서는 0으로 초기화하는 것이 표준입니다. ("grad가 none이라고 하는 게 더 직관적일 텐데 왜 영어로 선언하는지 여쭤보세요. ... 제로 초기화하는 게 맞다. 판단이 됩니다.")
* **no_grad, detach 관련**
  * 평가나 추론 시에는 반드시 no_grad 블록을 사용해야 하며, 그렇지 않으면 불필요한 메모리 사용과 속도 저하가 발생할 수 있습니다. ("성능 평가하거나 추론할 때는 반드시 노그레드 하셔야 된다는 얘기예요. 무조건 알겠습니까?")
  * detach는 특정 텐서를 계산 그래프에서 영구적으로 분리하는 것이므로, 분리 후에는 해당 텐서에 대한 미분이 전파되지 않습니다. ("detach는 영구적 분리예요. ... 분리된다로 끝나시면 될 것 같아요.")
* **신경망 구현 관련**
  * 부모 클래스의 생성자(super(). **init** )를 호출하지 않으면 에러가 발생합니다. 반드시 먼저 호출해야 합니다. ("이거 안 하면 에러예요. 이거 안 하면 뭐 발생한다고? 에러면요. 에러 발생합니다.")
  * 파이토치 모델에서 forward 메서드를 직접 호출하지 않고, 모델 객체에 입력을 바로 전달하면 자동으로 forward가 호출됩니다. ("forward 코드가 안 보여요. 알겠습니까? 이런 부분들을 좀 조심하셔야 된다는 얘기입니다.")
* **기타**
  * 신경망을 수동으로 구현할 때, 중간 노드의 기울기를 확인하려면 retain_grad를 미리 호출해야 하며, 그렇지 않으면 중간 노드의 gradient는 저장되지 않습니다. ("기본적으로 C grad는 none이에요. ... retain_grad를 미리 호출해 놓고 가야 C가 살아남습니다.")
